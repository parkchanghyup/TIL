{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798b1a24-19ae-4df5-916b-9d8b5cabbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deee2aca-3a79-43be-aa6f-08c78aa98769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 검증용 이미지 데이터, 어노테이션 데이터의 파일 경로 리스트 작성\n",
    "\n",
    "def make_datapath_list(rootpath):\n",
    "    \"\"\"\n",
    "    데이터 경로를 저장한 리스트 생성\n",
    "\n",
    "    파라미터\n",
    "    ---\n",
    "    rootpath : str\n",
    "        데이터 폴더 경로\n",
    "    \n",
    "    returns\n",
    "    ---\n",
    "    ret : train_img_list, train_anno_list, val_img_list, val_anno_list\n",
    "        데이터 경로를 저장한 리스트\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # 이미지 데이터와 어노테이션 파일의 경로 템플릿 작성\n",
    "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n",
    "    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n",
    "\n",
    "    # 학습 및 검증 파일 ID 휙득\n",
    "    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')\n",
    "    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n",
    "\n",
    "    # 학습 데이터의 이미지 파일과 어노테이션 파일의 경로 리스트 작성\n",
    "    train_img_list = list()\n",
    "    train_anno_list = list()\n",
    "\n",
    "    for line in open(train_id_names):\n",
    "        file_id = line.strip()  # 공백과 줄 바꿈 제거\n",
    "        img_path = (imgpath_template % file_id)  # 이미지 경로\n",
    "        anno_path = (annopath_template % file_id)  # 어노테이션 경로\n",
    "        train_img_list.append(img_path)  # 리스트에 추가\n",
    "        train_anno_list.append(anno_path)  # 리스트에 추가\n",
    "\n",
    "    # 검증 데이터의 이미지 파일과 어노테이션 파일의 경로 리스트 작성\n",
    "    val_img_list = list()\n",
    "    val_anno_list = list()\n",
    "\n",
    "    for line in open(val_id_names):\n",
    "        file_id = line.strip()  # 공백과 줄 바꿈 제거\n",
    "        img_path = (imgpath_template % file_id)  # 이미지 경로\n",
    "        anno_path = (annopath_template % file_id)  # 어노테이션 경로\n",
    "        val_img_list.append(img_path)  # 리스트에 추가\n",
    "        val_anno_list.append(anno_path)  # 리스트에 추가\n",
    "\n",
    "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "370d6eff-6575-49cd-99dc-395f9f3157e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/VOCdevkit/VOC2012/ImageSets/Main/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 테스트\u001b[39;00m\n\u001b[0;32m      2\u001b[0m rootpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/VOCdevkit/VOC2012/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m train_img_list, train_anno_list, val_img_list, val_anno_list \u001b[38;5;241m=\u001b[39m \u001b[43mmake_datapath_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrootpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_img_list[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m, in \u001b[0;36mmake_datapath_list\u001b[1;34m(rootpath)\u001b[0m\n\u001b[0;32m     28\u001b[0m train_img_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     29\u001b[0m train_anno_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_id_names\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     32\u001b[0m     file_id \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()  \u001b[38;5;66;03m# 공백과 줄 바꿈 제거\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m (imgpath_template \u001b[38;5;241m%\u001b[39m file_id)  \u001b[38;5;66;03m# 이미지 경로\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/VOCdevkit/VOC2012/ImageSets/Main/train.txt'"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "rootpath = './data/VOCdevkit/VOC2012/'\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
    "    rootpath)\n",
    "\n",
    "print(train_img_list[0])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55c4e5-5d04-4008-b512-9a3510627b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML 형식의 어노테이션을 리스트 형식으로 변환하는 클래스\n",
    "\n",
    "class Anno_xml2list(object):\n",
    "    \"\"\"\n",
    "    한 이미지의 XM 형식 어노테이션 데이터를 이미지 크기로 규격화하여 리스트 형식으로 변환\n",
    "\n",
    "    attributes\n",
    "    ---\n",
    "    classes : 리스트\n",
    "        VOC의 클래스명을 저장한 리스트\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classes):\n",
    "\n",
    "        self.classes = classes\n",
    "\n",
    "    def __call__(self, xml_path, width, height):\n",
    "        \"\"\"\n",
    "        한 이미지의 XML 형식 어노테이션 데이터를 이미지 크기로 규격화하여 리스트 형식으로 변환 \n",
    "\n",
    "        파라미터\n",
    "        ----------\n",
    "        xml_path : str\n",
    "            xml 파일 경로\n",
    "        width : int\n",
    "            대상 이미지 폭\n",
    "        height : int\n",
    "            대상 이미지 높이\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ret : [[xmin, ymin, xmax, ymax, label_ind], ... ]\n",
    "            물체의 어노테이션 데이터를 저장한 리스트. 이미지에 존재하는 물체 수만큼의 요소를 가진다.\n",
    "        \"\"\"\n",
    "\n",
    "        # 이미지 내 모든 물체의 어노테이션을 이 리스트에 저장\n",
    "        ret = []\n",
    "\n",
    "        # xml 파일 로드\n",
    "        xml = ET.parse(xml_path).getroot()\n",
    "\n",
    "        # 이미지 내 object 수만큼 반복\n",
    "        for obj in xml.iter('object'):\n",
    "\n",
    "            # 어노테이션에서 검지가 difficult로 설정된 것은 제외\n",
    "            difficult = int(obj.find('difficult').text)\n",
    "            if difficult == 1:\n",
    "                continue\n",
    "\n",
    "            # 한 물체의 어노테이션을 저장하는 리스트\n",
    "            bndbox = []\n",
    "\n",
    "            name = obj.find('name').text.lower().strip()  #  물체 이름\n",
    "            bbox = obj.find('bndbox')  # 바운딩 박스 정보\n",
    "\n",
    "            # 어노테이션의 xmin, ymin, xmax, ymax를 취득하고 0~1로 정규화\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "\n",
    "            for pt in (pts):\n",
    "                # VOC데이터는 원점이 (1,1) 이므로 1을 빼서 (0,0) 변환한다.\n",
    "                cur_pixel = int(bbox.find(pt).text) - 1\n",
    "\n",
    "                # 폭, 높이로 규격화\n",
    "                if pt == 'xmin' or pt == 'xmax':  # x 방향의 경우 폭으로 나눈다.\n",
    "                    cur_pixel /= width\n",
    "                else:  # y 방향의 경우 높이로 나눈다.\n",
    "                    cur_pixel /= height\n",
    "\n",
    "                bndbox.append(cur_pixel)\n",
    "\n",
    "            # 어노테이션 클래스명 index를 츼득하여 추가\n",
    "            label_idx = self.classes.index(name)\n",
    "            bndbox.append(label_idx)\n",
    "\n",
    "            #  res에 [xmin, ymin, xmax, label_ind]를 더한다\n",
    "            ret += [bndbox]\n",
    "\n",
    "        return np.array(ret)  # [[xmin, ymin, xmax, ymax, label_ind], ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf402c-0132-497a-9070-b9228f375193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "# 이미지 로드용으로 OpenCV tkdyd\n",
    "ind = 1\n",
    "image_file_path = val_img_list[ind]\n",
    "img = cv2.imread(image_file_path)  # 높이/폭/색RGB\n",
    "height, width, channels = img.shape  # 이미지 shape\n",
    "\n",
    "# 어노테이션을 리스트로 표시\n",
    "transform_anno(val_anno_list[ind], width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc61da-b6f5-4c76-bb4d-d32390370212",
   "metadata": {},
   "source": [
    "# 이미지와 어노테이션의 전처리를 실행하는 DataTransform 클래스 \n",
    "\r",
    "- \n",
    "데이터 transform 적용 시 이미지를 확장하는데, 그때 BBox정도를 동시에 환형해야함.\r",
    "- \n",
    "\r\n",
    "이미지 데이터를 불때올 떄 OpenCV를 사용하는데 OpenCv로 이미지를 불러올때는 높이/폭/색상(BGR)순으로 불러온온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c5e088-1b4c-41b5-bb4a-bcab89d50da4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 입력 이미지의 전처리 클래스\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_augumentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataTransform\u001b[39;00m():\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    이미지와 어노테이션의 전처리 클래스. 학습과 추론에서 다르게 작동한다.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    이미지 크기를 300x300으로 한다.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m        각 색상 채널의 평균 값\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\만들면서 배우는 파이토치\\2_objectdetection\\utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_augumentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mssd_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\만들면서 배우는 파이토치\\2_objectdetection\\utils\\data_augumentation.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 입력 이미지의 전처리 클래스\n",
    "from utils.data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n",
    "\n",
    "class DataTransform():\n",
    "    \"\"\"\n",
    "    이미지와 어노테이션의 전처리 클래스. 학습과 추론에서 다르게 작동한다.\n",
    "    이미지 크기를 300x300으로 한다.\n",
    "    학습 시에만 데이터의 이미지를 확장한다.\n",
    "\n",
    "    attributes\n",
    "    ---\n",
    "    input_size : int\n",
    "        리사이즈 대상 화상의 크기\n",
    "    color_mean : (B, G, R)\n",
    "        각 색상 채널의 평균 값\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, color_mean):\n",
    "        self.data_transform = {\n",
    "            'train': Compose([\n",
    "                ConvertFromInts(),  # int를 float32로 변환\n",
    "                ToAbsoluteCoords(),  # 어노테이션 데이터의 규격화 반환\n",
    "                PhotometricDistort(),  # 이미지의 색조 등 임의의 변화\n",
    "                Expand(color_mean),  # 이미지의 캔버스 확대\n",
    "                RandomSampleCrop(),  # 이미지내의 특정 부분 무작위 추출\n",
    "                RandomMirror(),  # 이미지 반전\n",
    "                ToPercentCoords(),  # 어노테이션 데이터를 0~1로 규격화\n",
    "                Resize(input_size),  # 이미지 크기를 input_size x input_size로 변형\n",
    "                SubtractMeans(color_mean)  # BGR 색상의 평균값 빼기\n",
    "            ]),\n",
    "            'val': Compose([\n",
    "                ConvertFromInts(),  # int를 float으로 변환\n",
    "                Resize(input_size),  # 화상 크기를 input_size x input_size로 변환\n",
    "                SubtractMeans(color_mean)  # BGR 색상의 평균값 빼기\n",
    "            ])\n",
    "        }\n",
    "\n",
    "    def __call__(self, img, phase, boxes, labels):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        phase : 'train' or 'val'\n",
    "            전처리 모드 지정\n",
    "        \"\"\"\n",
    "        return self.data_transform[phase](img, boxes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3df9f-64b8-44cb-97f3-0b7f0fbabccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동작 확인 \n",
    "\n",
    "# 1. 이미지 읽기\n",
    "image_file_path = train_img_list[0]\n",
    "img = cv2.imread(image_file_path)  # [높이/폭/색BGR]\n",
    "height, width, channels = img.shape  # 이미지 크기 휙득\n",
    "\n",
    "# 2. 어노테이션을 리스트로\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "anno_list = transform_anno(train_anno_list[0], width, height)\n",
    "\n",
    "# 3. 원본 표시\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "# 4. 전처리 클래스 작성\n",
    "color_mean = (104, 117, 123)  # BGR 색상의 평균 값\n",
    "input_size = 300  # 화상의 input 사이즈를 300 x 300 으로\n",
    "transform = DataTransform(input_size, color_mean)\n",
    "\n",
    "# 5. train 이미지 변환 후 출력\n",
    "phase = \"train\"\n",
    "img_transformed, boxes, labels = transform(\n",
    "    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
    "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 6. val 이미지 변환 후 출력\n",
    "phase = \"val\"\n",
    "img_transformed, boxes, labels = transform(\n",
    "    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
    "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b475f6-99a4-4908-ae7d-cf38415b4a89",
   "metadata": {},
   "source": [
    "# dataset 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0303cf7-b1c2-49bf-88f9-56109f0b1f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VOCDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    VOC2012의 Dataset을 만드는 클래스. 파이토치의 Dataset 클래스를 상속한다.\n",
    "\n",
    "    Attributes\n",
    "    ---\n",
    "    img_list : 리스트\n",
    "        이미지 경로를 저장한 리스트\n",
    "    anno_list : 리스트\n",
    "        어노테이션 겨ㅇ로를 저장한 리스트\n",
    "    phase : 'train' or 'test'\n",
    "        학습 또는 훈련 설정\n",
    "    transform : object\n",
    "        전처리 클래스의 인스턴스\n",
    "    transform_anno : object\n",
    "        xml 어노테이션을 리스트로 변환하는 인스턴스\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n",
    "        self.img_list = img_list\n",
    "        self.anno_list = anno_list\n",
    "        self.phase = phase  # train or val\n",
    "        self.transform = transform  # 이미지 변환\n",
    "        self.transform_anno = transform_anno  # 어노테이션 데이터를 xml에서 리스트로 변경\n",
    "\n",
    "    def __len__(self):\n",
    "        '''이미지 갯수 반환'''\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        전처리된 이미지의 텐서 형식 데이터와 어노테이션 반환\n",
    "        '''\n",
    "        im, gt, h, w = self.pull_item(index)\n",
    "        return im, gt\n",
    "\n",
    "    def pull_item(self, index):\n",
    "        '''\n",
    "        전처리한 이미지의 텐서 형식 데이터, 어노테이션, 이미지 높이, 폭 반환\n",
    "        '''\n",
    "\n",
    "        # 1. 이미지 로딩\n",
    "        image_file_path = self.img_list[index]\n",
    "        img = cv2.imread(image_file_path)  # 높이/넓이/색BGR\n",
    "        height, width, channels = img.shape  # 이미지 shape\n",
    "\n",
    "        # 2. xml 형식의 어노테이션 정보를 리스트에 저장\n",
    "        anno_file_path = self.anno_list[index]\n",
    "        anno_list = self.transform_anno(anno_file_path, width, height)\n",
    "\n",
    "        # 3. 전처리 실시\n",
    "        img, boxes, labels = self.transform(\n",
    "            img, self.phase, anno_list[:, :4], anno_list[:, 4])\n",
    "\n",
    "        # 색상 채널의 순서가 BGR이므로 RGB로 변경\n",
    "        # 높이/ 폭/ 색상채널의 순서를 색상채널/높이/폭으로 변경\n",
    "        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
    "\n",
    "        # BBox와 라벨을 세트로 한 np.array를 생성. 변수 이름 gt는 ground truth의 약칭\n",
    "        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
    "\n",
    "        return img, gt, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7c54a-d172-4266-aa8d-0180fd2b9d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터셋 작동 확인\n",
    "color_mean = (104, 117, 123)  # (BGR)의 색의 평균값\n",
    "input_size = 300  # 이미지 input 사이즈를 300x300으로 한다.\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "\n",
    "# 이미지 출력 예시\n",
    "val_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128ae1f-67a7-4bfa-8e9f-60a4d2d3a194",
   "metadata": {},
   "source": [
    "## DataLoader 구현\n",
    "\n",
    "object detection에서는 Dataloader를 조금 다르게 선언해야 한다.  \r\n",
    "이미지 데이터마다 dataset에서 꺼낼 어노테이션 데이터 정보, gt 변수의 크기(화상 내의 물체 수)가 다르다. gt는 리스트형 변수이고 요소 수는 이미지 속 물체 수 이다.  \n",
    " 각 요소는다섯 개의 변수 [xmin, ymin, xmax, ymax, class_index] 이다   .\r\n",
    "dataset에서 꺼내는 변수의 크기가 데이터마다 다르다면 DataLoader 클래스에서 기본적으로 사용하는 데이터 추출 함수인 collate_fn을 별도로 만들어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba009d6-6f75-4cbd-beb7-6c24c97f532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def od_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    dataset에서 꺼내는 어노테이션 데이터의 크기는 이미지마다 다르다.\n",
    "    이미지 내의 물체 수가 두 개이면 (2,5) 사이즈이지만 세 개이면 (3, 5)로 바뀐다.\n",
    "    변화에 대응하는 DataLoader를 만드는 collate_fn을 작성한다.\n",
    "    collate_fn은 파이토치 리스토로 mini-batch를 작성하는 함수이다. \n",
    "    미니 배치 분량의 화상이 나열된 리스트 변수 batch에 미니 배치 번호를 지정하는 차원을\n",
    "    가장 앞에 하나 추가하여 리스트 형태로 변형한다.\n",
    "    \"\"\"\n",
    "\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0]) # sample[0]는 이미지\n",
    "        targets.append(torch.FloatTensor(sample[1])) # sample[1]은 어노테이션 gt\n",
    "    \n",
    "    # imgs는 미니 배치 크기의 리스트\n",
    "    # 리스트 요소는 torch.Size([3, 300, 300])\n",
    "    # 이 리스트를 torch.Size([batch_num,3,300,300])의 텐서로 변환\n",
    "    imgs = torch.stack(imgs, dim = 0)\n",
    "\n",
    "    # tragets는 어노테이션의 정답인 gt 리스트\n",
    "    # 리스트 크기 = 미니 배치 크기\n",
    "    # targets 리스트의 요소는 [n,5]\n",
    "    # n은 이미지 마다 다르며 이미지 속 물체의 수\n",
    "    # 5는 [xmin, ymin, xmax, ymax, class_index]\n",
    "\n",
    "    return imgs, targets\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaafe44-3c2b-4860-83c9-0566d3343ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 테스트\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n",
    "\n",
    "# dict 형식으로 정리\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "\n",
    "# 동작 확인\n",
    "batch_iterator = iter(dataloaders_dict[\"val\"])  \n",
    "images, targets = next(batch_iterator)  # 첫 번째 요소 추출\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].size())  # 미니 배치 크기의 리스트, 각 요소는 [n,5], n은 물체 수\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272fce7-0986-45db-b9e5-a081fe863fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
