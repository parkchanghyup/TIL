{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "파이토치 딥러닝 3. 시맨틱 분할.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## semantic segmentation\n",
        "- `semantic segmentation`이란 한 장의 화상에 포함된 여러 물체의 영역과 이름을 픽셀 수준에서 지정하느 작업이다.\n",
        "- obejcet detection이 물체를 커다란 직사각형의 BBox로 묶었지만 시맨틱 분할에서는 픽셀 수준으로 **어디에서 어디까지 어떠한 클래스 물체인지** 라벨을 붙입니다. \n",
        "- 제조업의 흠집 탐지, 의료 영상 진단의 병변 감지, 자율 운전의 주변 환경 파악 등에서 시맨틱 분할 기술을 사용합니다. \n",
        "- 입력 : 이미지\n",
        "- 출력 : 각 픽셀이 속한 클래스의 라벨 정보\n",
        "- semantic segmentation 순서\n",
        "    1. 이미지 전처리(리사이즈, 정규화)\n",
        "    2. 이미지를 pspnet에입력 \n",
        "    3. pspnet 출력이 최댓값인 클래스 추출 \n",
        "        - (클래스수 x 475 x 475)의 배열 -> (475 x 475)의 배열\n",
        "    4. 3단계의 출력(475 x 475) 배열을 원본 이미지 크기로 복원 \n"
      ],
      "metadata": {
        "id": "abWr_6lRUQHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code"
      ],
      "metadata": {
        "id": "_Dv0jO4AVmlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/YutaroOgawa/pytorch_advanced.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm2gXlxGVrga",
        "outputId": "2baf46fc-8906-4153-f562-0da50a465614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch_advanced'...\n",
            "remote: Enumerating objects: 552, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 552 (delta 36), reused 61 (delta 31), pack-reused 479\u001b[K\n",
            "Receiving objects: 100% (552/552), 17.80 MiB | 31.98 MiB/s, done.\n",
            "Resolving deltas: 100% (295/295), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd pytorch_advanced/3_semantic_segmentation/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD3CyqEeV3YX",
        "outputId": "51b2a303-35f9-4ae4-eb34-0eca0d2ae817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch_advanced/3_semantic_segmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile\n",
        "\n",
        "# フォルダ「data」が存在しない場合は作成する\n",
        "data_dir = \"./data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "    # VOC2012のデータセットをここからダウンロードします\n",
        "# 時間がかかります（約15分）\n",
        "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
        "target_path = os.path.join(data_dir, \"VOCtrainval_11-May-2012.tar\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "    \n",
        "    tar = tarfile.TarFile(target_path)  # tarファイルを読み込み\n",
        "    tar.extractall(data_dir)  # tarを解凍\n",
        "    tar.close()  # tarファイルをクローズ\n",
        "    "
      ],
      "metadata": {
        "id": "qkYTC8r4Vl_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "3ZxjHXhpVaHW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfCBpUaZTTZ4"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 임포트\n",
        "import os.path as osp\n",
        "from PIL import Image\n",
        "\n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_datapath_list(rootpath):\n",
        "    \"\"\"\n",
        "    학습 및 검증용 화상 데이터와 어노테이션 데이터의 파일 경로 리스트 작성\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rootpath : str\n",
        "        데이터의 폴더 경로\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ret : train_img_list, train_anno_list, val_img_list, val_anno_list\n",
        "        데이터 경로를 저장한 리스트\n",
        "    \"\"\"\n",
        "\n",
        "    # 이미지 파일과 어노테이션 파일의 경로 템플릿 작성\n",
        "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n",
        "    annopath_template = osp.join(rootpath, 'SegmentationClass', '%s.png')\n",
        "\n",
        "    # 후련 및 검증 파일 각각의 ID(파일 이름) 취득\n",
        "    train_id_names = osp.join(rootpath + 'ImageSets/Segmentation/train.txt')\n",
        "    val_id_names = osp.join(rootpath + 'ImageSets/Segmentation/val.txt')\n",
        "\n",
        "    # 훈련 데이터의 이미지 파일과 어노테이션 파일의 경로 리스트 작성\n",
        "    train_img_list = list()\n",
        "    train_anno_list = list()\n",
        "\n",
        "    for line in open(train_id_names):\n",
        "        file_id = line.strip()  # 공백과 줄 바꿈 제거\n",
        "        img_path = (imgpath_template % file_id)  # 이미지 경로\n",
        "        anno_path = (annopath_template % file_id)  # 어노테이션 경로\n",
        "        train_img_list.append(img_path)\n",
        "        train_anno_list.append(anno_path)\n",
        "\n",
        "    # 검증 데이터의 이미지 파일과 어노테이션 파일의 경로 리스트 작성\n",
        "    val_img_list = list()\n",
        "    val_anno_list = list()\n",
        "\n",
        "    for line in open(val_id_names):\n",
        "        file_id = line.strip()  # 공백과 줄 바꿈\n",
        "        img_path = (imgpath_template % file_id)  # 이미지 경로\n",
        "        anno_path = (annopath_template % file_id)  # 어노테이션 경로\n",
        "        val_img_list.append(img_path)\n",
        "        val_anno_list.append(anno_path)\n",
        "\n",
        "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
      ],
      "metadata": {
        "id": "tVr8vpi0X6n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행 확인\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
        "    rootpath=rootpath)\n",
        "\n",
        "print(train_img_list[0])\n",
        "print(train_anno_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHrJNJShYdh0",
        "outputId": "ab8a1e3c-7060-41c3-e0bf-1d0f866445a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data/VOCdevkit/VOC2012/JPEGImages/2007_000032.jpg\n",
            "./data/VOCdevkit/VOC2012/SegmentationClass/2007_000032.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 처리 클래스와 데이터 확장 클래스\n",
        "from utils.data_augumentation import Compose, Scale, RandomRotation, RandomMirror, Resize, Normalize_Tensor\n",
        "\n",
        "\n",
        "class DataTransform():\n",
        "    \"\"\"\n",
        "    이미지와 어노테이션의 전처리 클래스. 학습과 검증 시 다르게 작동.\n",
        "    이미지 크기를 input_size x input_size로 한다.\n",
        "    검증 시 데이터 확장을 수행 한다.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    input_size : int\n",
        "        리사이즈 대상의 이미지 크기\n",
        "    color_mean : (R, G, B)\n",
        "        각 색상 채널의 평균값\n",
        "    color_std : (R, G, B)\n",
        "        각 색상 채널의 표준편차\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, color_mean, color_std):\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                Scale(scale=[0.5, 1.5]),  # 이미지 확대\n",
        "                RandomRotation(angle=[-10, 10]),  # 회전\n",
        "                RandomMirror(),  # 랜덤 미러\n",
        "                Resize(input_size),  # 리사이즈(input_size)\n",
        "                Normalize_Tensor(color_mean, color_std)  # 색상 정보의 표준화와 텐서로 변환\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                Resize(input_size),  # 리사이즈(input_size)\n",
        "                Normalize_Tensor(color_mean, color_std)  # 색상 정보의 표준화와 텐서로 변환\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, phase, img, anno_class_img):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            전처리 모드 지정\n",
        "        \"\"\"\n",
        "        return self.data_transform[phase](img, anno_class_img)"
      ],
      "metadata": {
        "id": "edCRnaxJYfe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VOCDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Voc2012의 Dataset을 만드는 클래스. 파이토치의 Dataset 클래스를 상속 받는다.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    img_list : list\n",
        "        어노테이션 경로를 저장한 리스트\n",
        "    anno_list : lsit\n",
        "        어노테이션 경로를 저장한 리스트\n",
        "    phase : 'train' or 'test'\n",
        "        학습 또는 훈련 지정\n",
        "    transform : object\n",
        "        전처리 클래스 인스턴스\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_list, anno_list, phase, transform):\n",
        "        self.img_list = img_list\n",
        "        self.anno_list = anno_list\n",
        "        self.phase = phase\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        '''이미지 갯수 반환'''\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        전처리한 화상의 텐서 형식 데이터와 어노테이션 휙득\n",
        "        '''\n",
        "        img, anno_class_img = self.pull_item(index)\n",
        "        return img, anno_class_img\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        '''이미지 텐서 형식 데이터와 어노테이션 휙득'''\n",
        "\n",
        "        # 1. 이미지 읽기\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = Image.open(image_file_path)   # [높이][폭][색RGB]\n",
        "\n",
        "        # 2. 어노테이션 이미지 읽기 \n",
        "        anno_file_path = self.anno_list[index]\n",
        "        anno_class_img = Image.open(anno_file_path)   # [높이][폭]\n",
        "\n",
        "        # 3. 전처리 적용\n",
        "        img, anno_class_img = self.transform(self.phase, img, anno_class_img)\n",
        "\n",
        "        return img, anno_class_img"
      ],
      "metadata": {
        "id": "OYK6_QuYY6eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동작 확인\n",
        "\n",
        "# (RGB)색의 평균치와 표준 편차\n",
        "color_mean = (0.485, 0.456, 0.406)\n",
        "color_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "# 데이터셋 작성\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
        "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
        "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "# 데이터 추출 예\n",
        "print(val_dataset.__getitem__(0)[0].shape)\n",
        "print(val_dataset.__getitem__(0)[1].shape)\n",
        "print(val_dataset.__getitem__(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-XEmRrgaH4W",
        "outputId": "02068d29-6c91-49cc-9803-9ffaa242e7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 475, 475])\n",
            "torch.Size([475, 475])\n",
            "(tensor([[[ 1.6667,  1.5125,  1.5639,  ...,  1.7523,  1.6667,  1.7009],\n",
            "         [ 1.5810,  1.4269,  1.4783,  ...,  1.7009,  1.6153,  1.6495],\n",
            "         [ 1.5639,  1.4098,  1.4440,  ...,  1.6838,  1.5982,  1.6324],\n",
            "         ...,\n",
            "         [-0.4739, -0.4911, -0.5424,  ...,  1.2557,  1.1872,  1.2214],\n",
            "         [-0.5596, -0.4911, -0.4911,  ...,  1.2385,  1.1872,  1.2214],\n",
            "         [-0.6281, -0.3883, -0.3369,  ...,  1.2385,  1.1872,  1.2214]],\n",
            "\n",
            "        [[ 1.8333,  1.6758,  1.7283,  ...,  1.9209,  1.8333,  1.8683],\n",
            "         [ 1.7458,  1.5882,  1.6408,  ...,  1.8683,  1.7808,  1.8158],\n",
            "         [ 1.7283,  1.5707,  1.6057,  ...,  1.8508,  1.7633,  1.7983],\n",
            "         ...,\n",
            "         [-0.5826, -0.6001, -0.6527,  ...,  1.4132,  1.3431,  1.3431],\n",
            "         [-0.6702, -0.6001, -0.6001,  ...,  1.3957,  1.3431,  1.3431],\n",
            "         [-0.7402, -0.4951, -0.4426,  ...,  1.3957,  1.3431,  1.3431]],\n",
            "\n",
            "        [[ 2.0474,  1.8905,  1.9428,  ...,  2.1346,  2.0474,  2.0823],\n",
            "         [ 1.9603,  1.8034,  1.8557,  ...,  2.0823,  1.9951,  2.0300],\n",
            "         [ 1.9428,  1.7860,  1.8208,  ...,  2.0648,  1.9777,  2.0125],\n",
            "         ...,\n",
            "         [-0.6367, -0.6541, -0.7064,  ...,  1.6291,  1.5594,  1.5768],\n",
            "         [-0.7238, -0.6541, -0.6541,  ...,  1.6117,  1.5594,  1.5768],\n",
            "         [-0.7936, -0.5495, -0.4973,  ...,  1.6117,  1.5594,  1.5768]]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 로더 작성"
      ],
      "metadata": {
        "id": "gT07pCmCaWbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터로더작성\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# dict로 오브젝트 정리\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "\n",
        "# 작동 확인\n",
        "batch_iterator = iter(dataloaders_dict[\"val\"])  # iterator로 변환\n",
        "imges, anno_class_imges = next(batch_iterator)  # 첫 번째 요소르 꺼냄\n",
        "print(imges.size())  # torch.Size([8, 3, 475, 475])\n",
        "print(anno_class_imges.size())  # torch.Size([8, 3, 475, 475])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_U4THQKaKCD",
        "outputId": "77655fb0-5904-421a-9987-07099a7f9fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3, 475, 475])\n",
            "torch.Size([8, 475, 475])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PSPNet 구성\n",
        "---\n",
        "PSPNet은 4개의 모듈인 `Feature, Pyramid Pooling, Decoder, AuxLoss`로 구성 되어 있다.\n",
        "1. Feature 모듈 (Encoder 모듈)\n",
        "    - 입력 이미지의 특징을 파악\n",
        "    - 출력 : 2048 x 60 x 60 (채널 x 높이 x 폭)\n",
        "    - 이미지의 특징을 파악한 2048ch과 이미지 크기가 60x60으로 변환됨.\n",
        "2. Pyramid Pooling 모듈 \n",
        "    - 어떠한 픽셀의 물체 라벨을 구하려면 다양한 크기로 해당 픽셀 주변 정보가 필요\n",
        "    - Pyramid Pooling 모듈에서는 네 가지 크기의 특징량 맵을 준비\n",
        "    - `이미지 전체 크기`, `이미지 절반 크기`, `이미지 1/3 크기`, `이미지 1/6 크기`\n",
        "    - 출력 : 4096 x 60 x 60\n",
        "3. Decoder 모듈(업샘플링 모듈)\n",
        "    - Pyramid Pooling 모듈의 출력을 21 x 60 x 60(클래스 수 x 높이 x 폭)텐서로 변환\n",
        "    - 21 x 60 x 60 (클래스 수 x 높이 x 폭)으로 변환된 텐서를 원 이미지 크기에 맞도록 변환(21 x 475 x 475)\n",
        "    - 출력 : 21 x 475 x 475\n",
        "    - 추론 시 Decoder 모듈의 출력으로 출력에 대한 최대 확률의 물체 클래스를 찾아 각 픽셀의 라벨을 결정.\n",
        "4. AuxLoss (보조 loss)\n",
        "    - 손실함수 게산을 보조 \n",
        "    - Feature 모듈로 중간 정보를 빼내 입력 데이터로 사용\n",
        "    - Decoder 모듈처럼 각 픽셀에 대응하는 물체 라벨 추정 클래스 분류 실행\n",
        "    - 입력 : 1024 x 60 x 60, 출력 : 21 x 475 x 475\n",
        "    - AuxLoss 모듈은 Feature 층의 중간까지 결과로 시맨틱 분할을 실시하여 분류 정확도는 떨어진다. 하지만 오차 역전파법 수행 시 Feature 층 중간까지의 네트워크 파라미터 학습을 보조하는 역할을 한다. \n",
        "    - 학습시에는 AuxLoss 모듈을 사용하지만 추론 시에는 AuxLoss 모듈의 출력은 사용하지 않고 Decoder 모듈의 출력만으로 semantic segmantation 을 실행한다.\n"
      ],
      "metadata": {
        "id": "Er1jXcXPajnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PSPNet 클래스\n",
        "- PSPNet 클래스의 메서드는 forward 뿐.\n",
        "- 순서대로 각 모듈의 서브 네트워크를 실행함. \n",
        "- 단 AuxLoss 모듈을 Feature 모듈의 네 번째 서브 네트워크 feature_dilated_res_1의 뒤에 넣어 output_aux변수로 출력을 작성\n",
        "- forward 메서드 끝에 메인 output과 output_aux를 반환\n"
      ],
      "metadata": {
        "id": "6glN8b2rfJ1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "lhznIQ9Eai6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PSPNet(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(PSPNet, self).__init__()\n",
        "\n",
        "        # 파라미터 설정\n",
        "        block_config = [3, 4, 6, 3]  # resnet50\n",
        "        img_size = 475\n",
        "        img_size_8 = 60  # ima/size의 1/8로 설정\n",
        "\n",
        "        # 네 개의 모듈을 구성하는 서브 네트워크 준비\n",
        "        self.feature_conv = FeatureMap_convolution()\n",
        "        self.feature_res_1 = ResidualBlockPSP(\n",
        "            n_blocks=block_config[0], in_channels=128, mid_channels=64, out_channels=256, stride=1, dilation=1)\n",
        "        self.feature_res_2 = ResidualBlockPSP(\n",
        "            n_blocks=block_config[1], in_channels=256, mid_channels=128, out_channels=512, stride=2, dilation=1)\n",
        "        self.feature_dilated_res_1 = ResidualBlockPSP(\n",
        "            n_blocks=block_config[2], in_channels=512, mid_channels=256, out_channels=1024, stride=1, dilation=2)\n",
        "        self.feature_dilated_res_2 = ResidualBlockPSP(\n",
        "            n_blocks=block_config[3], in_channels=1024, mid_channels=512, out_channels=2048, stride=1, dilation=4)\n",
        "\n",
        "        self.pyramid_pooling = PyramidPooling(in_channels=2048, pool_sizes=[\n",
        "            6, 3, 2, 1], height=img_size_8, width=img_size_8)\n",
        "\n",
        "        self.decode_feature = DecodePSPFeature(\n",
        "            height=img_size, width=img_size, n_classes=n_classes)\n",
        "\n",
        "        self.aux = AuxiliaryPSPlayers(\n",
        "            in_channels=1024, height=img_size, width=img_size, n_classes=n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_conv(x)\n",
        "        x = self.feature_res_1(x)\n",
        "        x = self.feature_res_2(x)\n",
        "        x = self.feature_dilated_res_1(x)\n",
        "\n",
        "        output_aux = self.aux(x)  # Feature 모듈의 중간을 Aux 모듈로\n",
        "\n",
        "        x = self.feature_dilated_res_2(x)\n",
        "\n",
        "        x = self.pyramid_pooling(x)\n",
        "        output = self.decode_feature(x)\n",
        "\n",
        "        return (output, output_aux)"
      ],
      "metadata": {
        "id": "pm55F_L1gK_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 모듈 설명 및 구현\n",
        "- Feature 모듈은 다섯 개의 서브 네트워크인 `FeatureMap_convolution`, `두 개의 ResidualBlockPSP`, `두 개의 dilated(확장판) ResidualBlockPSP`로 구성됨\n",
        "- 네 번째 서브 네트워크인 dilated 판 ResidualBlcokPSP의 출력 텐서 1024 x60 x 60(ch x 높이 x 폭)이 AuxLoss 모듈로 출력되는 점에 주의 \n",
        "- AuxLoss모듈에서는 이 출력 텐서로 픽셀별 클래스를 분류하고 그 손실 값을 Feature 모듈의 전반부 네 개의 서브 네트워크를 학습하는데 사용."
      ],
      "metadata": {
        "id": "MTr5QunegY7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  서브 네트워크 FeatureMap_convolution\n",
        "- 입력 : 3 x 475 x 475, 출력 : 128 x 119 x 119\n",
        "- `convolution, batch noramalization, Relu를 한 층으로하는 conv2dBatchNormRlue가 세 층`, `max polling 한 층` 총 4층으로 이뤄져있음"
      ],
      "metadata": {
        "id": "1OqAuVYzhFn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class conv2DBatchNormRelu(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):\n",
        "        super(conv2DBatchNormRelu, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
        "                              kernel_size, stride, padding, dilation, bias=bias)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # inplace를 설정으로 입력을 저장하지 않고 출력을 게산하여 메모리 절약\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.batchnorm(x)\n",
        "        outputs = self.relu(x)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "XA7ItTNSgYF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureMap_convolution(nn.Module):\n",
        "    def __init__(self):\n",
        "        '''構成するネットワークを用意'''\n",
        "        super(FeatureMap_convolution, self).__init__()\n",
        "\n",
        "        # 합성곱 층 1\n",
        "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 3, 64, 3, 2, 1, 1, False\n",
        "        self.cbnr_1 = conv2DBatchNormRelu(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
        "\n",
        "        # 합성곱 층 2\n",
        "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 64, 64, 3, 1, 1, 1, False\n",
        "        self.cbnr_2 = conv2DBatchNormRelu(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
        "\n",
        "        # 합성곱 층 3\n",
        "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 64, 128, 3, 1, 1, 1, False\n",
        "        self.cbnr_3 = conv2DBatchNormRelu(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
        "\n",
        "        # 합성곱 층 4\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cbnr_1(x)\n",
        "        x = self.cbnr_2(x)\n",
        "        x = self.cbnr_3(x)\n",
        "        outputs = self.maxpool(x)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "5x66qfjeiH48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResidualBlockPSP\n",
        "- bottleNeckPSP과 연속된 bottleNeckIdentifyPSP 로 이뤄져 있다."
      ],
      "metadata": {
        "id": "BXGTwSEjiP8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlockPSP(nn.Sequential):\n",
        "    def __init__(self, n_blocks, in_channels, mid_channels, out_channels, stride, dilation):\n",
        "        super(ResidualBlockPSP, self).__init__()\n",
        "\n",
        "        # bottleNeckPSP 준비\n",
        "        self.add_module(\n",
        "            \"block1\",\n",
        "            bottleNeckPSP(in_channels, mid_channels,\n",
        "                          out_channels, stride, dilation)\n",
        "        )\n",
        "\n",
        "        # bottleNeckIdentifyPS 반복 준비\n",
        "        for i in range(n_blocks - 1):\n",
        "            self.add_module(\n",
        "                \"block\" + str(i+2),\n",
        "                bottleNeckIdentifyPSP(\n",
        "                    out_channels, mid_channels, stride, dilation)\n",
        "            )"
      ],
      "metadata": {
        "id": "7G06VM6-iPm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class conv2DBatchNorm(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):\n",
        "        super(conv2DBatchNorm, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
        "                              kernel_size, stride, padding, dilation, bias=bias)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        outputs = self.batchnorm(x)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "u33kGBZdioFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class bottleNeckPSP(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, out_channels, stride, dilation):\n",
        "        super(bottleNeckPSP, self).__init__()\n",
        "\n",
        "        self.cbr_1 = conv2DBatchNormRelu(\n",
        "            in_channels, mid_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
        "        self.cbr_2 = conv2DBatchNormRelu(\n",
        "            mid_channels, mid_channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n",
        "        self.cb_3 = conv2DBatchNorm(\n",
        "            mid_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
        "\n",
        "        # 스킵 결합\n",
        "        self.cb_residual = conv2DBatchNorm(\n",
        "            in_channels, out_channels, kernel_size=1, stride=stride, padding=0, dilation=1, bias=False)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))\n",
        "        residual = self.cb_residual(x)\n",
        "        return self.relu(conv + residual)"
      ],
      "metadata": {
        "id": "3mloLKtZjBbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class bottleNeckIdentifyPSP(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, stride, dilation):\n",
        "        super(bottleNeckIdentifyPSP, self).__init__()\n",
        "\n",
        "        self.cbr_1 = conv2DBatchNormRelu(\n",
        "            in_channels, mid_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
        "        self.cbr_2 = conv2DBatchNormRelu(\n",
        "            mid_channels, mid_channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)\n",
        "        self.cb_3 = conv2DBatchNorm(\n",
        "            mid_channels, in_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))\n",
        "        residual = x\n",
        "        return self.relu(conv + residual)"
      ],
      "metadata": {
        "id": "z28KGC0GjEzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pyramid Pooling 모둘 설명 및 구현\n",
        "- 입력 : 2048 x 60 x 60, 출력 : 4096 x 60 x 60\n",
        "- 입력을 이용하여 총 5개의 특칭맵을 추출\n",
        "- 5개 중 네 개는 출력이 각각 6, 3 ,2, 1인 Adptive Average Pooling 층으로 보내짐\n",
        "- Average Pooling 층을 통과한 텐서는 `conv2DBatchNormRelu` 클래스를 지나 Upsample층에 도달\n",
        "- upsample 층에서 모든 특징량의 크기를 입력 크기와 같은 60 x 60 크기로 확대\n",
        "- 네 개의 분기는 채널 수가 각각 512개이고 512 x 4 + 2048 = 4096이 되어 `Pyramid Pooling`모듈의 출력 텐서 크기는 4096 x 90 x 90이다.\n",
        "- 위 출력 텐서를 Decoder로 보냄."
      ],
      "metadata": {
        "id": "KwY9l5lHjLg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PyramidPooling(nn.Module):\n",
        "    def __init__(self, in_channels, pool_sizes, height, width):\n",
        "        super(PyramidPooling, self).__init__()\n",
        "\n",
        "        # forward에서 사용하는 이미지 크기\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        # 각 합성곱 층의 출력 채널 수\n",
        "        out_channels = int(in_channels / len(pool_sizes))\n",
        "\n",
        "        # 각 합성곱 층 작성\n",
        "        # 다음은 for 무능로 구현하는 것이 좋지만 이해를 돕기 위해 하드코딩\n",
        "        # pool_sizes: [6, 3, 2, 1]\n",
        "        self.avpool_1 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[0])\n",
        "        self.cbr_1 = conv2DBatchNormRelu(\n",
        "            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
        "\n",
        "        self.avpool_2 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[1])\n",
        "        self.cbr_2 = conv2DBatchNormRelu(\n",
        "            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
        "\n",
        "        self.avpool_3 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[2])\n",
        "        self.cbr_3 = conv2DBatchNormRelu(\n",
        "            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
        "\n",
        "        self.avpool_4 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[3])\n",
        "        self.cbr_4 = conv2DBatchNormRelu(\n",
        "            in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out1 = self.cbr_1(self.avpool_1(x))\n",
        "        out1 = F.interpolate(out1, size=(\n",
        "            self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "        out2 = self.cbr_2(self.avpool_2(x))\n",
        "        out2 = F.interpolate(out2, size=(\n",
        "            self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "        out3 = self.cbr_3(self.avpool_3(x))\n",
        "        out3 = F.interpolate(out3, size=(\n",
        "            self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "        out4 = self.cbr_4(self.avpool_4(x))\n",
        "        out4 = F.interpolate(out4, size=(\n",
        "            self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "        # 최종 결합시킹 dim=1로 채널 수 차원에서 결합\n",
        "        output = torch.cat([x, out1, out2, out3, out4], dim=1)\n"
      ],
      "metadata": {
        "id": "dGcGySjEjE-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder, AuxLoss 모듈 설명 및 구현\n",
        "- Pyramid Pooling 또는 Feature 모듈에서 출력된 텐서 정보를 Decode(해석)함\n",
        "- 텐서 정보를 읽은 후 픽셀별로 물체 라벨을 클래스 분류로 추정하고 마지막 이미지 크기를 원래 475 x475로 업샘플링 한다. \n",
        "- Decoder과 AuxLoss 모듈은 모두 동일한 네트워크 구성이다.\n",
        "- `conv2DBatchNormRelu`클래스를 통과한 후 dropout 층을 지나 합성곱층을 통과 하여 텐서 크기가 21 x 60 x 60이 된다. \n",
        "- 마지막으로 UpSample 층을 지나 PSPNet 입력 화상의 크기였던 475 사이즈로 확대한다. \n",
        "- Upsample은 F.interpolate 연산으로 구현"
      ],
      "metadata": {
        "id": "C5IrUzPKk6Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecodePSPFeature(nn.Module):\n",
        "    def __init__(self, height, width, n_classes):\n",
        "        super(DecodePSPFeature, self).__init__()\n",
        "\n",
        "        # forward에 사용하는 이미지 크기\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        self.cbr = conv2DBatchNormRelu(\n",
        "            in_channels=4096, out_channels=512, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
        "        self.dropout = nn.Dropout2d(p=0.1)\n",
        "        self.classification = nn.Conv2d(\n",
        "            in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cbr(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.classification(x)\n",
        "        output = F.interpolate(\n",
        "            x, size=(self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "9F76lqIqk5vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AuxiliaryPSPlayers(nn.Module):\n",
        "    def __init__(self, in_channels, height, width, n_classes):\n",
        "        super(AuxiliaryPSPlayers, self).__init__()\n",
        "\n",
        "        # forward에 사용하는 이미지 크기\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        self.cbr = conv2DBatchNormRelu(\n",
        "            in_channels=in_channels, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
        "        self.dropout = nn.Dropout2d(p=0.1)\n",
        "        self.classification = nn.Conv2d(\n",
        "            in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cbr(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.classification(x)\n",
        "        output = F.interpolate(\n",
        "            x, size=(self.height, self.width), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "ZLvIk425ltD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의\n",
        "net = PSPNet(n_classes=21)\n",
        "net"
      ],
      "metadata": {
        "id": "xvtOCUPGltWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0916d845-d21b-4900-ac8d-8bd835cfee64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PSPNet(\n",
              "  (feature_conv): FeatureMap_convolution(\n",
              "    (cbnr_1): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (cbnr_2): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (cbnr_3): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_res_1): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (feature_res_2): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block4): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (feature_dilated_res_1): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block4): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block5): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block6): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (feature_dilated_res_2): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (pyramid_pooling): PyramidPooling(\n",
              "    (avpool_1): AdaptiveAvgPool2d(output_size=6)\n",
              "    (cbr_1): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (avpool_2): AdaptiveAvgPool2d(output_size=3)\n",
              "    (cbr_2): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (avpool_3): AdaptiveAvgPool2d(output_size=2)\n",
              "    (cbr_3): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (avpool_4): AdaptiveAvgPool2d(output_size=1)\n",
              "    (cbr_4): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (decode_feature): DecodePSPFeature(\n",
              "    (cbr): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (aux): AuxiliaryPSPlayers(\n",
              "    (cbr): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fine 튜닝 학습 및 점증"
      ],
      "metadata": {
        "id": "YS9nA7sAptwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "w12IkrEbp0VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup seeds\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "metadata": {
        "id": "35wlBoiDpzFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터로더 생성"
      ],
      "metadata": {
        "id": "j8QS3tUtp2DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
        "\n",
        "# 파일 경로 릿트 생성\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
        "    rootpath=rootpath)\n",
        "\n",
        "# 데이터셋 작성\n",
        "# (RGB)색의 평균값과 표준편차\n",
        "color_mean = (0.485, 0.456, 0.406)\n",
        "color_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
        "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
        "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "# DataLoader작성\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# dict로 저장\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
      ],
      "metadata": {
        "id": "5cw0IbK-p1Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.pspnet import PSPNet\n",
        "\n",
        "# 파인튜닝으로 PSPNet 생성\n",
        "# ADE20k 데이터셋의 학습된 모델을 사용하며 ADE20k 클래스 수는 150\n",
        "net = PSPNet(n_classes=150)\n",
        "\n",
        "# ADE20K학습된 파라미터 읽기\n",
        "state_dict = torch.load(\"./pspnet50_ADE20K.pth\")\n",
        "net.load_state_dict(state_dict)\n",
        "\n",
        "# 분류용 합성곱 층을 출력 수 21로 바꾼다.\n",
        "n_classes = 21\n",
        "net.decode_feature.classification = nn.Conv2d(\n",
        "    in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "net.aux.classification = nn.Conv2d(\n",
        "    in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "# 교체한 합성곱 층 초기화. 활성화 함수는 시그모이드 함수이므로 Xavier 사용\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:  # 바이어스가 있는 경우\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "# https://drive.google.com/open?id=12eN6SpnawYuQmD1k9VgVW3QSgPR6hICc (weight 다운)\n",
        "net.decode_feature.classification.apply(weights_init)\n",
        "net.aux.classification.apply(weights_init)\n",
        "\n",
        "\n",
        "print('네트워크 설정 완료 : pre-trained 가중치 load')"
      ],
      "metadata": {
        "id": "xOdfJWvtqE5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 손실함수 정의\n",
        "- cross entropy loss  와 auxloss 손실 합을 총 손실로함\n",
        "- AuxLOss는 계수 0.4를 곱하여 가중치를 메인 손실보다 작게한다."
      ],
      "metadata": {
        "id": "bFy-MWpOrPgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실함수 정의\n",
        "class PSPLoss(nn.Module):\n",
        "    \"\"\"PSPNet 손실함수 클래스 \"\"\"\n",
        "\n",
        "    def __init__(self, aux_weight=0.4):\n",
        "        super(PSPLoss, self).__init__()\n",
        "        self.aux_weight = aux_weight  # aux_loss가중치\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\"\n",
        "        손실함수 계산\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        outputs : PSPNet 출력(tuple)\n",
        "            (output=torch.Size([num_batch, 21, 475, 475]), output_aux=torch.Size([num_batch, 21, 475, 475]))。\n",
        "\n",
        "        targets : [num_batch, 475, 475]\n",
        "            정답 어노테이션 정보\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss : 텐서\n",
        "            손실 값\n",
        "        \"\"\"\n",
        "\n",
        "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
        "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
        "\n",
        "        return loss+self.aux_weight*loss_aux\n",
        "\n",
        "\n",
        "criterion = PSPLoss(aux_weight=0.4)"
      ],
      "metadata": {
        "id": "ZoDypieYqhCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 스케줄러로 에폭별 학습 비율 변경\n",
        "---\n",
        "- 입력에 가까운 모듈의 학습률은 작게, 교체한 합성곱 층을 가진 Deocder와 AuxLoss 모듈은 크게 설정\n",
        "- 에폭 별 학습률을 변화시키는 스케쥴러 활용 \n",
        "- lambda_epoch함수는 최대 에폭 수를 30으로하고 에폭을 거칠 때마다 학습률이 서서히 작아짐.\n",
        "- return 하는 값을 옵티마이저 학습률에 곱한다. "
      ],
      "metadata": {
        "id": "jP-_I3YBrhUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파인튜닝이므로 학습률 작게\n",
        "optimizer = optim.SGD([\n",
        "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
        "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
        "], momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "\n",
        "# 스케쥴러 설정\n",
        "def lambda_epoch(epoch):\n",
        "    max_epoch = 30\n",
        "    return math.pow((1-epoch/max_epoch), 0.9)\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
      ],
      "metadata": {
        "id": "DNF9dTcprgn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Jb6TF9SWk0vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train 정의\n",
        "1. 스케쥴러 적용\n",
        "2. 멀티플 미니 배치 적용\n",
        "    - 손실 loss의 계산과 경사를 구하는 backward를 여러 번 실행 .\n",
        "    - 각 파라미터에 복수의 미니 배치로 계산된 경사의 합계를 구하여 optimizer.step()를 실행한 후 파라미터를 갱신하는 기술"
      ],
      "metadata": {
        "id": "wzg0C7djk1YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 학습시키는 함수\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
        "\n",
        "    # Gpu를 사용할 수 있는지 확인\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"사용 장치：\", device)\n",
        "\n",
        "    # 네트워크를 gpu로\n",
        "    net.to(device)\n",
        "\n",
        "    # 네트워크가 어느 정도 고정되면 고속화\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # 이미지 갯수\n",
        "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
        "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # iteratior의 카운터 설정\n",
        "    iteration = 1\n",
        "    logs = []\n",
        "\n",
        "    # multiple minibatch\n",
        "    batch_multiplier = 3\n",
        "\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # 시작 시간 저장\n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "        epoch_train_loss = 0.0  # epoch손실합\n",
        "        epoch_val_loss = 0.0  # epoch손실합\n",
        "\n",
        "        print('-------------')\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-------------')\n",
        "\n",
        "        # epoch 학습 및 검증\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # 모델을 학습 모드로\n",
        "                scheduler.step()  # optimizer  스케쥴러 갱신\n",
        "                optimizer.zero_grad()\n",
        "                print('（train）')\n",
        "\n",
        "            else:\n",
        "                if((epoch+1) % 5 == 0):\n",
        "                    net.eval()   # 모델을 검증 모드로\n",
        "                    print('-------------')\n",
        "                    print('（val）')\n",
        "                else:\n",
        "                    # 검증은 다섯 번 중 한번만 수행\n",
        "                    continue\n",
        "\n",
        "            # 데이터 로더에서 미니 배치를 꺼내 수행\n",
        "            count = 0  # multiple minibatch\n",
        "            for imges, anno_class_imges in dataloaders_dict[phase]:\n",
        "                # 미니 배치 크기가 1이면 배치 정규화에서 오류가 발생하여 피한다.\n",
        "                \n",
        "                if imges.size()[0] == 1:\n",
        "                    continue\n",
        "\n",
        "                # gpu를 사용할 수 있으면 gpu에 데이터를 보낸다\n",
        "                imges = imges.to(device)\n",
        "                anno_class_imges = anno_class_imges.to(device)\n",
        "\n",
        "                \n",
        "                # multiple minibatch로 파라미터 갱신\n",
        "                if (phase == 'train') and (count == 0):\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    count = batch_multiplier\n",
        "\n",
        "                # 순전파 계산\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(imges)\n",
        "                    loss = criterion(\n",
        "                        outputs, anno_class_imges.long()) / batch_multiplier\n",
        "\n",
        "                    # 훈련시 역전파\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()  # 경사 계산\n",
        "                        count -= 1  # multiple minibatch\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iter에 한 번 loss 표시\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print('반복 {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
        "                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
        "                            t_iter_start = time.time()\n",
        "\n",
        "                        epoch_train_loss += loss.item() * batch_multiplier\n",
        "                        iteration += 1\n",
        "\n",
        "                    # 검증\n",
        "                    else:\n",
        "                        epoch_val_loss += loss.item() * batch_multiplier\n",
        "\n",
        "        # epoch의phase별 손실과 정답률\n",
        "        t_epoch_finish = time.time()\n",
        "        print('-------------')\n",
        "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
        "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
        "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "        # 로그 저장\n",
        "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss /\n",
        "                     num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}\n",
        "        logs.append(log_epoch)\n",
        "        df = pd.DataFrame(logs)\n",
        "        df.to_csv(\"log_output.csv\")\n",
        "\n",
        "    # 마지막 모델 저장\n",
        "    torch.save(net.state_dict(), 'weights/pspnet50_' +\n",
        "               str(epoch+1) + '.pth')"
      ],
      "metadata": {
        "id": "dxTAbwS-r_SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 및 검증 실행\n",
        "num_epochs = 30\n",
        "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "bBIQtgkXlrOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inference \n",
        "- 파일 경로 리스트를 만듦\n",
        "- VOC 데이터셋 이미지가 아니라 승마 이미지에 대해 추론하며 어노테이션 이미지를 한장 사용\n",
        "- 어노테이션 이미지를 사용하는 이유는 2가지가 있다.\n",
        "    1. 어노에티셔 이미지가 없으면 전처리 클래스의 함수가 제대로 작동하지 않는다. 실제로 추론에 사용하지 않지만 더미 데이터로서 함수에 전달 된다.\n",
        "    2. 어노테이션 이미지에서 색상 팔레트 정보를 추출하지 않으면 물체 라벨에 해당하는 생상 정보가 존재하지 않게 된다. "
      ],
      "metadata": {
        "id": "VEKWqMXbmcIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd pytorch_advanced/2_objectdetection/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb23IVkZm5y0",
        "outputId": "0a88cc3a-98ea-407b-ef19-77eacc0deb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch_advanced/2_objectdetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.dataloader import make_datapath_list, DataTransform\n",
        "\n",
        "\n",
        "# 파일 경로 리스트 작성|\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
        "    rootpath=rootpath)"
      ],
      "metadata": {
        "id": "U-k_0cj2mOx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#psp net 준비\n",
        "from utils.pspnet import PSPNet\n",
        "\n",
        "net = PSPNet(n_classes=21)\n",
        "\n",
        "# 학습된 파라미터 읽기\n",
        "state_dict = torch.load(\"./weights/pspnet50_30.pth\",\n",
        "                        map_location={'cuda:0': 'cpu'})\n",
        "net.load_state_dict(state_dict)\n",
        "\n",
        "print('네트워크 설정 및 가중치 로딩 완료.')"
      ],
      "metadata": {
        "id": "qwDEVBkvm3NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 1. 원본 이미지 표시\n",
        "image_file_path = \"./data/cowboy-757575_640.jpg\"\n",
        "img = Image.open(image_file_path)   # [높이][폭][색RGB]\n",
        "img_width, img_height = img.size\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# 2. 전처리 클래스 작성\n",
        "color_mean = (0.485, 0.456, 0.406)\n",
        "color_std = (0.229, 0.224, 0.225)\n",
        "transform = DataTransform(\n",
        "    input_size=475, color_mean=color_mean, color_std=color_std)\n",
        "\n",
        "# 3. 전처리\n",
        "# 적당한 어노테이션 이미지를 준비하여 생상 팔레트 정보 추출\n",
        "anno_file_path = val_anno_list[0]\n",
        "anno_class_img = Image.open(anno_file_path)   # [높이][폭]\n",
        "p_palette = anno_class_img.getpalette()\n",
        "phase = \"val\"\n",
        "img, anno_class_img = transform(phase, img, anno_class_img)\n",
        "\n",
        "\n",
        "# 4. PSPNet로 추론\n",
        "net.eval()\n",
        "x = img.unsqueeze(0)  # 미니 배치화：torch.Size([1, 3, 475, 475])\n",
        "outputs = net(x)\n",
        "y = outputs[0]  # AuxLoss 는 무시  y의 크기 : torch.Size([1, 21, 475, 475])\n",
        "\n",
        "\n",
        "# 5. PSPNet 출력으로 최대 클래스를 구하여 생상 팔레트 형식으로 이미지 크기를 원래대로 되돌림.\n",
        "y = y[0].detach().numpy()  # y：torch.Size([1, 21, 475, 475])\n",
        "y = np.argmax(y, axis=0)\n",
        "anno_class_img = Image.fromarray(np.uint8(y), mode=\"P\")\n",
        "anno_class_img = anno_class_img.resize((img_width, img_height), Image.NEAREST)\n",
        "anno_class_img.putpalette(p_palette)\n",
        "plt.imshow(anno_class_img)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 6. 이미지를 투과시켜 겹친다.\n",
        "trans_img = Image.new('RGBA', anno_class_img.size, (0, 0, 0, 0))\n",
        "anno_class_img = anno_class_img.convert('RGBA')  # 색상 팔레트 형식을 RGBA로 변경\n",
        "\n",
        "for x in range(img_width):\n",
        "    for y in range(img_height):\n",
        "        # 추론 결과 화상의 픽셀 데이터 취득\n",
        "        pixel = anno_class_img.getpixel((x, y))\n",
        "        r, g, b, a = pixel\n",
        "\n",
        "        # (0, 0, 0)의 배경이라면 그대로 투과\n",
        "        if pixel[0] == 0 and pixel[1] == 0 and pixel[2] == 0:\n",
        "            continue\n",
        "        else:\n",
        "            # 그 외 색상은 준비된 화상에 픽셀 기록\n",
        "            trans_img.putpixel((x, y), (r, g, b, 150))\n",
        "            # 투과율을 150으로 지정\n",
        "\n",
        "img = Image.open(image_file_path)   # [높이][폭][색RGB]\n",
        "result = Image.alpha_composite(img.convert('RGBA'), trans_img)\n",
        "plt.imshow(result)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ph8rcz1UnKD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "시맨틱 분할의 정확성을 높이려면 학습 에폭 수를 늘려야함 \n"
      ],
      "metadata": {
        "id": "d4ss6K5rn5I7"
      }
    }
  ]
}