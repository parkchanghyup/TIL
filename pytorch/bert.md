---
### ì•„ì§ ì‘ì„± ì™„ë£Œ ì „
---

# ë¬¸ì œì  

1. ìµœëŒ€ ê¸¸ì´ê°€ 512 ê¹Œì§€ë°–ì— ì•ˆë˜ëŠ” ê²ƒì¸ê°€ ?
2. Kobert modelì„ ì‚¬ìš©í•˜ë©´ labels ì—ëŸ¬ê°€ ë°œìƒí•¨ . .
3. 


<br>
<br>

# **ì¤€ë¹„ ì‚¬í•­**


```python
#!pip3 install kobert-transformers
!pip3 install sentencepiece
```

    Requirement already satisfied: kobert-transformers in /usr/local/lib/python3.7/dist-packages (0.4.1)
    Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from kobert-transformers) (1.8.0+cu101)
    Requirement already satisfied: transformers>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from kobert-transformers) (4.4.2)
    Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->kobert-transformers) (1.19.5)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->kobert-transformers) (3.7.4.3)
    Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers) (2.23.0)
    Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers) (4.41.1)
    Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers) (20.9)
    Requirement already satisfied: importlib-metadata; python_version < "3.8" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers) (3.7.2)
    Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers) (0.0.43)
    Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers) (0.10.1)
    Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers) (3.0.12)
    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers) (2019.12.20)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (2.10)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (1.24.3)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (3.0.4)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (2020.12.5)
    Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.9.1->kobert-transformers) (2.4.7)
    Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < "3.8"->transformers>=2.9.1->kobert-transformers) (3.4.1)
    Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.9.1->kobert-transformers) (1.15.0)
    Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.9.1->kobert-transformers) (1.0.1)
    Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.9.1->kobert-transformers) (7.1.2)
    Collecting sentencepiece
    [?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 8.3MB/s 
    [?25hInstalling collected packages: sentencepiece
    Successfully installed sentencepiece-0.1.95
    


```python
import torch
from transformers import BertModel
import sentencepiece as spm
from tokenization_kobert import KoBertTokenizer
```


```python
model = BertModel.from_pretrained('monologg/kobert',num_labels=2)
tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')
```


```python
import tensorflow as tf
import torch

from transformers import BertTokenizer
from transformers import BertForSequenceClassification, AdamW, BertConfig
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
import random
import time
import datetime
```

<br>
<br>

# **ë°ì´í„° ë¡œë“œ**


```python
# ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì •ë¶„ì„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
!git clone https://github.com/e9t/nsmc.git
```

    Cloning into 'nsmc'...
    remote: Enumerating objects: 14763, done.[K
    remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763[K
    Receiving objects: 100% (14763/14763), 56.19 MiB | 19.53 MiB/s, done.
    Resolving deltas: 100% (1749/1749), done.
    Checking out files: 100% (14737/14737), done.
    

ë°•ì€ì •ë‹˜ì˜ ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì •ë¶„ì„ ë°ì´í„°ë¥¼ Githubì—ì„œ ë‹¤ìš´ë¡œë“œ í•©ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ nsmc ë””ë ‰í† ë¦¬ì— ìˆëŠ” ratings_train.txtì™€ ratings_test.txtë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. 
<br>
<br>
<br>


```python
# ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡
!ls nsmc -la
```

    total 38632
    drwxr-xr-x 5 root root     4096 Mar 25 13:46 .
    drwxr-xr-x 1 root root     4096 Mar 25 13:46 ..
    drwxr-xr-x 2 root root     4096 Mar 25 13:46 code
    drwxr-xr-x 8 root root     4096 Mar 25 13:46 .git
    -rw-r--r-- 1 root root  4893335 Mar 25 13:46 ratings_test.txt
    -rw-r--r-- 1 root root 14628807 Mar 25 13:46 ratings_train.txt
    -rw-r--r-- 1 root root 19515078 Mar 25 13:46 ratings.txt
    drwxr-xr-x 2 root root   458752 Mar 25 13:46 raw
    -rw-r--r-- 1 root root     2596 Mar 25 13:46 README.md
    -rw-r--r-- 1 root root    36746 Mar 25 13:46 synopses.json
    


```python
# íŒë‹¤ìŠ¤ë¡œ í›ˆë ¨ì…‹ê³¼ í…ŒìŠ¤íŠ¸ì…‹ ë°ì´í„° ë¡œë“œ
train = pd.read_csv("nsmc/ratings_train.txt", sep='\t')
test = pd.read_csv("nsmc/ratings_test.txt", sep='\t')

print(train.shape)
print(test.shape)
```

    (150000, 3)
    (50000, 3)
    

í›ˆë ¨ì…‹ 150,000ê°œì™€ í…ŒìŠ¤íŠ¸ì…‹ 50,000ê°œì˜ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.
<br>
<br>
<br>


```python
# í›ˆë ¨ì…‹ì˜ ì•ë¶€ë¶„ ì¶œë ¥
train.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>document</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9976970</td>
      <td>ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3819312</td>
      <td>í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10265843</td>
      <td>ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9045019</td>
      <td>êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6483659</td>
      <td>ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5403919</td>
      <td>ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™”.ã…‹ã…‹ã…‹...ë³„ë°˜ê°œë„ ì•„ê¹Œì›€.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7797314</td>
      <td>ì›ì‘ì˜ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>9443947</td>
      <td>ë³„ ë°˜ê°œë„ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€..ì •ë§ ë°œë¡œí•´ë„ ê·¸ê²ƒë³´ë‹¨...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>7156791</td>
      <td>ì•¡ì…˜ì´ ì—†ëŠ”ë°ë„ ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ëŠ” ì˜í™”</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>5912145</td>
      <td>ì™œì¼€ í‰ì ì´ ë‚®ì€ê±´ë°? ê½¤ ë³¼ë§Œí•œë°.. í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜?</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



idëŠ” íšŒì›ì •ë³´, documentëŠ” ë¦¬ë·° ë¬¸ì¥ì…ë‹ˆë‹¤. labelì´ 0ì´ë©´ ë¶€ì •, 1ì´ë©´ ê¸ì •ìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤. idëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— documentì™€ labelë§Œ ì¶”ì¶œí•˜ê² ìŠµë‹ˆë‹¤. 

<br>
<br>

# **ì „ì²˜ë¦¬ - í›ˆë ¨ì…‹**


```python
# ë¦¬ë·° ë¬¸ì¥ ì¶”ì¶œ
sentences = train['document']
sentences[:10]
```




    0                                  ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬
    1                    í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜
    2                                    ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤
    3                        êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •
    4    ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...
    5        ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™”.ã…‹ã…‹ã…‹...ë³„ë°˜ê°œë„ ì•„ê¹Œì›€.
    6                                ì›ì‘ì˜ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤.
    7    ë³„ ë°˜ê°œë„ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€..ì •ë§ ë°œë¡œí•´ë„ ê·¸ê²ƒë³´ë‹¨...
    8                               ì•¡ì…˜ì´ ì—†ëŠ”ë°ë„ ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ëŠ” ì˜í™”
    9        ì™œì¼€ í‰ì ì´ ë‚®ì€ê±´ë°? ê½¤ ë³¼ë§Œí•œë°.. í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜?
    Name: document, dtype: object




```python
# BERTì˜ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
sentences = ["[CLS] " + str(sentence) + " [SEP]" for sentence in sentences]
sentences[:10]
```




    ['[CLS] ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬ [SEP]',
     '[CLS] í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜ [SEP]',
     '[CLS] ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤ [SEP]',
     '[CLS] êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì • [SEP]',
     '[CLS] ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ë˜ìŠ¤íŠ¸ê°€ ë„ˆë¬´ë‚˜ë„ ì´ë»ë³´ì˜€ë‹¤ [SEP]',
     '[CLS] ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™”.ã…‹ã…‹ã…‹...ë³„ë°˜ê°œë„ ì•„ê¹Œì›€. [SEP]',
     '[CLS] ì›ì‘ì˜ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤. [SEP]',
     '[CLS] ë³„ ë°˜ê°œë„ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€..ì •ë§ ë°œë¡œí•´ë„ ê·¸ê²ƒë³´ë‹¨ ë‚«ê²Ÿë‹¤ ë‚©ì¹˜.ê°ê¸ˆë§Œë°˜ë³µë°˜ë³µ..ì´ë“œë¼ë§ˆëŠ” ê°€ì¡±ë„ì—†ë‹¤ ì—°ê¸°ëª»í•˜ëŠ”ì‚¬ëŒë§Œëª¨ì—¿ë„¤ [SEP]',
     '[CLS] ì•¡ì…˜ì´ ì—†ëŠ”ë°ë„ ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ëŠ” ì˜í™” [SEP]',
     '[CLS] ì™œì¼€ í‰ì ì´ ë‚®ì€ê±´ë°? ê½¤ ë³¼ë§Œí•œë°.. í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜? [SEP]']



![ëŒ€ì²´ í…ìŠ¤íŠ¸](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)

BERTì˜ ì…ë ¥ì€ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ í˜•ì‹ì…ë‹ˆë‹¤. Classificationì„ ëœ»í•˜ëŠ” [CLS] ì‹¬ë³¼ì´ ì œì¼ ì•ì— ì‚½ì…ë©ë‹ˆë‹¤. íŒŒì¸íŠœë‹ì‹œ ì¶œë ¥ì—ì„œ ì´ ìœ„ì¹˜ì˜ ê°’ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ë¥¼ í•©ë‹ˆë‹¤. [SEP]ì€ Seperationì„ ê°€ë¦¬í‚¤ëŠ”ë°, ë‘ ë¬¸ì¥ì„ êµ¬ë¶„í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ ì˜ˆì œì—ì„œëŠ” ë¬¸ì¥ì´ í•˜ë‚˜ì´ë¯€ë¡œ [SEP]ë„ í•˜ë‚˜ë§Œ ë„£ìŠµë‹ˆë‹¤.
<br>
<br>
<br>


```python
# ë¼ë²¨ ì¶”ì¶œ
labels = train['label'].values
labels
```




    array([0, 1, 0, ..., 0, 1, 0])




```python
# BERTì˜ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„ë¦¬
#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)
tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]

print (sentences[0])
print (tokenized_texts[0])
```

    [CLS] ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬ [SEP]
    ['[CLS]', 'â–ì•„', 'â–ë”', 'ë¹™', '.', '.', 'â–ì§„ì§œ', 'â–ì§œ', 'ì¦', 'ë‚˜', 'ë„¤ìš”', 'â–ëª©ì†Œë¦¬', '[SEP]']
    

BERTëŠ” í˜•íƒœì†Œë¶„ì„ìœ¼ë¡œ í† í°ì„ ë¶„ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. WordPieceë¼ëŠ” í†µê³„ì ì¸ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•œ ë‹¨ì–´ë‚´ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ê¸€ìë“¤ì„ ë¶™ì—¬ì„œ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì–¸ì–´ì— ìƒê´€ì—†ì´ í† í°ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì‹ ì¡°ì–´ ê°™ì´ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•˜ê¸°ë„ ì¢‹ìŠµë‹ˆë‹¤. 

ìœ„ì˜ ê²°ê³¼ì—ì„œ ## ê¸°í˜¸ëŠ” ì• í† í°ê³¼ ì´ì–´ì§„ë‹¤ëŠ” í‘œì‹œì…ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ì—¬ëŸ¬ ì–¸ì–´ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“  'bert-base-multilingual-cased'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ í•œê¸€ë„ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
<br>
<br>
<br>


```python
# ì…ë ¥ í† í°ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
MAX_LEN = 128

# í† í°ì„ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜
input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]

# ë¬¸ì¥ì„ MAX_LEN ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³ , ëª¨ìë€ ë¶€ë¶„ì„ íŒ¨ë”© 0ìœ¼ë¡œ ì±„ì›€
input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

input_ids[0]
```




    array([   2, 3093, 1698, 6456,   54,   54, 4368, 4396, 7316, 5655, 5703,
           2073,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0])



ë³´í†µ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ëŠ” í† í° ìì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„ë² ë”© ë ˆì´ì–´ì—ëŠ” í† í°ì„ ìˆ«ìë¡œ ëœ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. BERTì˜ í† í¬ë‚˜ì´ì €ëŠ” {ë‹¨ì–´í† í°:ì¸ë±ìŠ¤}ë¡œ êµ¬ì„±ëœ ë‹¨ì–´ì‚¬ì „ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì°¸ì¡°í•˜ì—¬ í† í°ì„ ì¸ë±ìŠ¤ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.
<br>
<br>
<br>


```python
# ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì´ˆê¸°í™”
attention_masks = []

# ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ íŒ¨ë”©ì´ ì•„ë‹ˆë©´ 1, íŒ¨ë”©ì´ë©´ 0ìœ¼ë¡œ ì„¤ì •
# íŒ¨ë”© ë¶€ë¶„ì€ BERT ëª¨ë¸ì—ì„œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ ì†ë„ í–¥ìƒ
for seq in input_ids:
    seq_mask = [float(i>0) for i in seq]
    attention_masks.append(seq_mask)

print(attention_masks[0])
```

    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    


```python
# í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,
                                                                                    labels, 
                                                                                    random_state=42, 
                                                                                    test_size=0.1)

# ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬
train_masks, validation_masks, _, _ = train_test_split(attention_masks, 
                                                       input_ids,
                                                       random_state=42, 
                                                       test_size=0.1)

# ë°ì´í„°ë¥¼ íŒŒì´í† ì¹˜ì˜ í…ì„œë¡œ ë³€í™˜
train_inputs = torch.tensor(train_inputs)
train_labels = torch.tensor(train_labels)
train_masks = torch.tensor(train_masks)
validation_inputs = torch.tensor(validation_inputs)
validation_labels = torch.tensor(validation_labels)
validation_masks = torch.tensor(validation_masks)				

print(train_inputs[0])
print(train_labels[0])
print(train_masks[0])
print(validation_inputs[0])
print(validation_labels[0])
print(validation_masks[0])
```

    tensor([   2, 4384, 6903, 2421, 5761, 1458, 3060, 7782, 5512, 6999,   54,   54,
            6689, 4013, 5330, 4368, 3977, 6881, 6857,   54,   54, 6834, 5937, 4207,
            7848, 6999,   54,   54, 6993, 5689,   54,   54, 6041,   54,   54, 5482,
            6116, 4998, 5808, 5771, 6999,   54,   54, 5538, 6037, 5655, 3220, 6115,
            5937, 2372, 4396, 7316, 5669, 5772,   54, 3990, 7824, 3751, 3234, 5940,
            2417, 6150, 7828, 3394, 5495, 6999,   54,   54, 1788, 6136, 7019, 6664,
              46,  517, 7772, 6493, 3194, 5770, 1267, 7782, 6604, 5784, 5176, 7953,
            7843,   54,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0])
    tensor(0)
    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0.])
    tensor([  2, 517,   0,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0])
    tensor(1)
    tensor([1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0.])
    


```python
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ
batch_size = 32

# íŒŒì´í† ì¹˜ì˜ DataLoaderë¡œ ì…ë ¥, ë§ˆìŠ¤í¬, ë¼ë²¨ì„ ë¬¶ì–´ ë°ì´í„° ì„¤ì •
# í•™ìŠµì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë§Œí¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)
```

<br>
<br>

# **ì „ì²˜ë¦¬ - í…ŒìŠ¤íŠ¸ì…‹**


```python
# ë¦¬ë·° ë¬¸ì¥ ì¶”ì¶œ
sentences = test['document']
sentences[:10]
```




    0                                                  êµ³ ã…‹
    1                                 GDNTOPCLASSINTHECLUB
    2               ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„
    3                     ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„... ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”....
    4    3Dë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë°.. ì™œ 3Dë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ ??
    5                                   ìŒì•…ì´ ì£¼ê°€ ëœ, ìµœê³ ì˜ ìŒì•…ì˜í™”
    6                                              ì§„ì •í•œ ì“°ë ˆê¸°
    7             ë§ˆì¹˜ ë¯¸êµ­ì• ë‹ˆì—ì„œ íŠ€ì–´ë‚˜ì˜¨ë“¯í•œ ì°½ì˜ë ¥ì—†ëŠ” ë¡œë´‡ë””ìì¸ë¶€í„°ê°€,ê³ ê°œë¥¼ ì –ê²Œí•œë‹¤
    8    ê°ˆìˆ˜ë¡ ê°œíŒë˜ê°€ëŠ” ì¤‘êµ­ì˜í™” ìœ ì¹˜í•˜ê³  ë‚´ìš©ì—†ìŒ í¼ì¡ë‹¤ ëë‚¨ ë§ë„ì•ˆë˜ëŠ” ë¬´ê¸°ì— ìœ ì¹˜í•œc...
    9       ì´ë³„ì˜ ì•„í””ë’¤ì— ì°¾ì•„ì˜¤ëŠ” ìƒˆë¡œìš´ ì¸ì—°ì˜ ê¸°ì¨ But, ëª¨ë“  ì‚¬ëŒì´ ê·¸ë ‡ì§€ëŠ” ì•Šë„¤..
    Name: document, dtype: object




```python
# BERTì˜ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
sentences = ["[CLS] " + str(sentence) + " [SEP]" for sentence in sentences]
sentences[:10]
```




    ['[CLS] êµ³ ã…‹ [SEP]',
     '[CLS] GDNTOPCLASSINTHECLUB [SEP]',
     '[CLS] ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„ [SEP]',
     '[CLS] ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„... ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”.... [SEP]',
     '[CLS] 3Dë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë°.. ì™œ 3Dë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ ?? [SEP]',
     '[CLS] ìŒì•…ì´ ì£¼ê°€ ëœ, ìµœê³ ì˜ ìŒì•…ì˜í™” [SEP]',
     '[CLS] ì§„ì •í•œ ì“°ë ˆê¸° [SEP]',
     '[CLS] ë§ˆì¹˜ ë¯¸êµ­ì• ë‹ˆì—ì„œ íŠ€ì–´ë‚˜ì˜¨ë“¯í•œ ì°½ì˜ë ¥ì—†ëŠ” ë¡œë´‡ë””ìì¸ë¶€í„°ê°€,ê³ ê°œë¥¼ ì –ê²Œí•œë‹¤ [SEP]',
     '[CLS] ê°ˆìˆ˜ë¡ ê°œíŒë˜ê°€ëŠ” ì¤‘êµ­ì˜í™” ìœ ì¹˜í•˜ê³  ë‚´ìš©ì—†ìŒ í¼ì¡ë‹¤ ëë‚¨ ë§ë„ì•ˆë˜ëŠ” ë¬´ê¸°ì— ìœ ì¹˜í•œcgë‚¨ë¬´ ì•„ ê·¸ë¦½ë‹¤ ë™ì‚¬ì„œë…ê°™ì€ ì˜í™”ê°€ ì´ê±´ 3ë¥˜ì•„ë¥˜ì‘ì´ë‹¤ [SEP]',
     '[CLS] ì´ë³„ì˜ ì•„í””ë’¤ì— ì°¾ì•„ì˜¤ëŠ” ìƒˆë¡œìš´ ì¸ì—°ì˜ ê¸°ì¨ But, ëª¨ë“  ì‚¬ëŒì´ ê·¸ë ‡ì§€ëŠ” ì•Šë„¤.. [SEP]']




```python
# ë¼ë²¨ ì¶”ì¶œ
labels = test['label'].values
labels
```




    array([1, 0, 0, ..., 0, 0, 0])




```python
# BERTì˜ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„ë¦¬
#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)
tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]

print (sentences[0])
print (tokenized_texts[0])
```

    [CLS] êµ³ ã…‹ [SEP]
    ['[CLS]', 'â–', 'êµ³', 'â–', 'á„', '[SEP]']
    


```python
# ì…ë ¥ í† í°ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
MAX_LEN = 128

# í† í°ì„ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜
input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]

# ë¬¸ì¥ì„ MAX_LEN ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³ , ëª¨ìë€ ë¶€ë¶„ì„ íŒ¨ë”© 0ìœ¼ë¡œ ì±„ì›€
input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

input_ids[0]
```




    array([   2,  517, 5515,  517,  492,    3,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0])




```python
# ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì´ˆê¸°í™”
attention_masks = []

# ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ íŒ¨ë”©ì´ ì•„ë‹ˆë©´ 1, íŒ¨ë”©ì´ë©´ 0ìœ¼ë¡œ ì„¤ì •
# íŒ¨ë”© ë¶€ë¶„ì€ BERT ëª¨ë¸ì—ì„œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ ì†ë„ í–¥ìƒ
for seq in input_ids:
    seq_mask = [float(i>0) for i in seq]
    attention_masks.append(seq_mask)

print(attention_masks[0])
```

    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    


```python
# ë°ì´í„°ë¥¼ íŒŒì´í† ì¹˜ì˜ í…ì„œë¡œ ë³€í™˜
test_inputs = torch.tensor(input_ids)
test_labels = torch.tensor(labels)
test_masks = torch.tensor(attention_masks)

print(test_inputs[0])
print(test_labels[0])
print(test_masks[0])
```

    tensor([   2,  517, 5515,  517,  492,    3,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
               0,    0,    0,    0,    0,    0,    0,    0])
    tensor(1)
    tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0.])
    


```python
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ
batch_size = 32

# íŒŒì´í† ì¹˜ì˜ DataLoaderë¡œ ì…ë ¥, ë§ˆìŠ¤í¬, ë¼ë²¨ì„ ë¬¶ì–´ ë°ì´í„° ì„¤ì •
# í•™ìŠµì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë§Œí¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
test_data = TensorDataset(test_inputs, test_masks, test_labels)
test_sampler = RandomSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)
```

<br>
<br>

# **ëª¨ë¸ ìƒì„±**


```python
# GPU ë””ë°”ì´ìŠ¤ ì´ë¦„ êµ¬í•¨
device_name = tf.test.gpu_device_name()

# GPU ë””ë°”ì´ìŠ¤ ì´ë¦„ ê²€ì‚¬
if device_name == '/device:GPU:0':
    print('Found GPU at: {}'.format(device_name))
else:
    raise SystemError('GPU device not found')
```

    Found GPU at: /device:GPU:0
    


```python
# ë””ë°”ì´ìŠ¤ ì„¤ì •
if torch.cuda.is_available():    
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print('No GPU available, using the CPU instead.')
```

    There are 1 GPU(s) available.
    We will use the GPU: Tesla P100-PCIE-16GB
    


```python
# ë¶„ë¥˜ë¥¼ ìœ„í•œ BERT ëª¨ë¸ ìƒì„±
model = BertForSequenceClassification.from_pretrained("bert-base-multilingual-cased", num_labels=2)
model.cuda()
```

    Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
    - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
    




    BertForSequenceClassification(
      (bert): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    )



![ëŒ€ì²´ í…ìŠ¤íŠ¸](http://www.mccormickml.com/assets/BERT/padding_and_mask.png)

ì‚¬ì „í›ˆë ¨ëœ BERTëŠ” ë‹¤ì–‘í•œ ë¬¸ì œë¡œ ì „ì´í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ í•œ ë¬¸ì¥ì„ ë¶„ë¥˜í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜í™”ë¦¬ë·° ë¬¸ì¥ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ë©´, ê¸ì •/ë¶€ì •ìœ¼ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì¶œë ¥ì—ì„œ [CLS] ìœ„ì¹˜ì¸ ì²« ë²ˆì§¸ í† í°ì— ìƒˆë¡œìš´ ë ˆì´ì–´ë¥¼ ë¶™ì—¬ì„œ íŒŒì¸íŠœë‹ì„ í•©ë‹ˆë‹¤. Huggning FaceëŠ” BertForSequenceClassification() í•¨ìˆ˜ë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
<br>
<br>
<br>


```python
# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer = AdamW(model.parameters(),
                  lr = 2e-5, # í•™ìŠµë¥ 
                  eps = 1e-8 # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ epsilon ê°’
                )

# ì—í­ìˆ˜
epochs = 4

# ì´ í›ˆë ¨ ìŠ¤í… : ë°°ì¹˜ë°˜ë³µ íšŸìˆ˜ * ì—í­
total_steps = len(train_dataloader) * epochs

# ì²˜ìŒì— í•™ìŠµë¥ ì„ ì¡°ê¸ˆì”© ë³€í™”ì‹œí‚¤ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)
```

<br>
<br>

# **ëª¨ë¸ í•™ìŠµ**


```python
# ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜
def flat_accuracy(preds, labels):
    
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    return np.sum(pred_flat == labels_flat) / len(labels_flat)
```


```python
# ì‹œê°„ í‘œì‹œ í•¨ìˆ˜
def format_time(elapsed):

    # ë°˜ì˜¬ë¦¼
    elapsed_rounded = int(round((elapsed)))
    
    # hh:mm:ssìœ¼ë¡œ í˜•íƒœ ë³€ê²½
    return str(datetime.timedelta(seconds=elapsed_rounded))
```


```python
# ì¬í˜„ì„ ìœ„í•´ ëœë¤ì‹œë“œ ê³ ì •
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
model.zero_grad()

# ì—í­ë§Œí¼ ë°˜ë³µ
for epoch_i in range(0, epochs):
    
    # ========================================
    #               Training
    # ========================================
    
    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')

    # ì‹œì‘ ì‹œê°„ ì„¤ì •
    t0 = time.time()

    # ë¡œìŠ¤ ì´ˆê¸°í™”
    total_loss = 0

    # í›ˆë ¨ëª¨ë“œë¡œ ë³€ê²½
    model.train()
        
    # ë°ì´í„°ë¡œë”ì—ì„œ ë°°ì¹˜ë§Œí¼ ë°˜ë³µí•˜ì—¬ ê°€ì ¸ì˜´
    for step, batch in enumerate(train_dataloader):
        # ê²½ê³¼ ì •ë³´ í‘œì‹œ
        if step % 500 == 0 and not step == 0:
            elapsed = format_time(time.time() - t0)
            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

        # ë°°ì¹˜ë¥¼ GPUì— ë„£ìŒ
        batch = tuple(t.to(device) for t in batch)
        
        # ë°°ì¹˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        b_input_ids, b_input_mask, b_labels = batch

        # Forward ìˆ˜í–‰                
        outputs = model(b_input_ids, 
                        token_type_ids=None, 
                        attention_mask=b_input_mask
                        ,labels=b_labels)
        
        # ë¡œìŠ¤ êµ¬í•¨
        loss = outputs[0]

        # ì´ ë¡œìŠ¤ ê³„ì‚°
        total_loss += loss.item()

        # Backward ìˆ˜í–‰ìœ¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        loss.backward()

        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        optimizer.step()

        # ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ í•™ìŠµë¥  ê°ì†Œ
        scheduler.step()

        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        model.zero_grad()

    # í‰ê·  ë¡œìŠ¤ ê³„ì‚°
    avg_train_loss = total_loss / len(train_dataloader)            

    print("")
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(format_time(time.time() - t0)))
        
    # ========================================
    #               Validation
    # ========================================

    print("")
    print("Running Validation...")

    #ì‹œì‘ ì‹œê°„ ì„¤ì •
    t0 = time.time()

    # í‰ê°€ëª¨ë“œë¡œ ë³€ê²½
    model.eval()

    # ë³€ìˆ˜ ì´ˆê¸°í™”
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0

    # ë°ì´í„°ë¡œë”ì—ì„œ ë°°ì¹˜ë§Œí¼ ë°˜ë³µí•˜ì—¬ ê°€ì ¸ì˜´
    for batch in validation_dataloader:
        # ë°°ì¹˜ë¥¼ GPUì— ë„£ìŒ
        batch = tuple(t.to(device) for t in batch)
        
        # ë°°ì¹˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        b_input_ids, b_input_mask, b_labels = batch
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì•ˆí•¨
        with torch.no_grad():     
            # Forward ìˆ˜í–‰
            outputs = model(b_input_ids, 
                            token_type_ids=None, 
                            attention_mask=b_input_mask)
        
        # ë¡œìŠ¤ êµ¬í•¨
        logits = outputs[0]

        # CPUë¡œ ë°ì´í„° ì´ë™
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        
        # ì¶œë ¥ ë¡œì§“ê³¼ ë¼ë²¨ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ ê³„ì‚°
        tmp_eval_accuracy = flat_accuracy(logits, label_ids)
        eval_accuracy += tmp_eval_accuracy
        nb_eval_steps += 1

    print("  Accuracy: {0:.2f}".format(eval_accuracy/nb_eval_steps))
    print("  Validation took: {:}".format(format_time(time.time() - t0)))

print("")
print("Training complete!")
```

    
    ======== Epoch 1 / 4 ========
    Training...
      Batch   500  of  4,219.    Elapsed: 0:03:06.
      Batch 1,000  of  4,219.    Elapsed: 0:06:12.
      Batch 1,500  of  4,219.    Elapsed: 0:09:18.
      Batch 2,000  of  4,219.    Elapsed: 0:12:24.
      Batch 2,500  of  4,219.    Elapsed: 0:15:30.
      Batch 3,000  of  4,219.    Elapsed: 0:18:36.
      Batch 3,500  of  4,219.    Elapsed: 0:21:42.
      Batch 4,000  of  4,219.    Elapsed: 0:24:48.
    
      Average training loss: 0.72
      Training epcoh took: 0:26:09
    
    Running Validation...
      Accuracy: 0.50
      Validation took: 0:00:57
    
    ======== Epoch 2 / 4 ========
    Training...
      Batch   500  of  4,219.    Elapsed: 0:03:06.
      Batch 1,000  of  4,219.    Elapsed: 0:06:12.
      Batch 1,500  of  4,219.    Elapsed: 0:09:17.
      Batch 2,000  of  4,219.    Elapsed: 0:12:23.
      Batch 2,500  of  4,219.    Elapsed: 0:15:29.
    


    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    <ipython-input-90-2c226948779a> in <module>()
         67 
         68         # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
    ---> 69         model.zero_grad()
         70 
         71     # í‰ê·  ë¡œìŠ¤ ê³„ì‚°
    

    /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in zero_grad(self, set_to_none)
       1510                     else:
       1511                         p.grad.requires_grad_(False)
    -> 1512                     p.grad.zero_()
       1513 
       1514     def share_memory(self: T) -> T:
    

    KeyboardInterrupt: 


ì—í­ë§ˆë‹¤ í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ì„ ë°˜ë³µí•˜ì—¬ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 

<br>
<br>

# **í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€**


```python
#ì‹œì‘ ì‹œê°„ ì„¤ì •
t0 = time.time()

# í‰ê°€ëª¨ë“œë¡œ ë³€ê²½
model.eval()

# ë³€ìˆ˜ ì´ˆê¸°í™”
eval_loss, eval_accuracy = 0, 0
nb_eval_steps, nb_eval_examples = 0, 0

# ë°ì´í„°ë¡œë”ì—ì„œ ë°°ì¹˜ë§Œí¼ ë°˜ë³µí•˜ì—¬ ê°€ì ¸ì˜´
for step, batch in enumerate(test_dataloader):
    # ê²½ê³¼ ì •ë³´ í‘œì‹œ
    if step % 100 == 0 and not step == 0:
        elapsed = format_time(time.time() - t0)
        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))

    # ë°°ì¹˜ë¥¼ GPUì— ë„£ìŒ
    batch = tuple(t.to(device) for t in batch)
    
    # ë°°ì¹˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ
    b_input_ids, b_input_mask, b_labels = batch
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì•ˆí•¨
    with torch.no_grad():     
        # Forward ìˆ˜í–‰
        outputs = model(b_input_ids, 
                        token_type_ids=None, 
                        attention_mask=b_input_mask)
    
    # ë¡œìŠ¤ êµ¬í•¨
    logits = outputs[0]

    # CPUë¡œ ë°ì´í„° ì´ë™
    logits = logits.detach().cpu().numpy()
    label_ids = b_labels.to('cpu').numpy()
    
    # ì¶œë ¥ ë¡œì§“ê³¼ ë¼ë²¨ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ ê³„ì‚°
    tmp_eval_accuracy = flat_accuracy(logits, label_ids)
    eval_accuracy += tmp_eval_accuracy
    nb_eval_steps += 1

print("")
print("Accuracy: {0:.2f}".format(eval_accuracy/nb_eval_steps))
print("Test took: {:}".format(format_time(time.time() - t0)))
```

í…ŒìŠ¤íŠ¸ì…‹ì˜ ì •í™•ë„ê°€ 87%ì…ë‹ˆë‹¤. <BERT í†ºì•„ë³´ê¸°> ë¸”ë¡œê·¸ì—ì„œëŠ” ê°™ì€ ë°ì´í„°ë¡œ 88.7%ë¥¼ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ê±°ê¸°ì„œëŠ” í•œê¸€ ì½”í¼ìŠ¤ë¡œ ì‚¬ì „í›ˆë ¨ì„ í•˜ì—¬ ìƒˆë¡œìš´ ëª¨ë¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ë°˜ë©´ì— ìš°ë¦¬ëŠ” BERTì˜ ê¸°ë³¸ ëª¨ë¸ì¸ bert-base-multilingual-casedë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— ë” ì„±ëŠ¥ì´ ë‚®ì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

<br>
<br>

# **ìƒˆë¡œìš´ ë¬¸ì¥ í…ŒìŠ¤íŠ¸**


```python
# ì…ë ¥ ë°ì´í„° ë³€í™˜
def convert_input_data(sentences):

    # BERTì˜ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„ë¦¬
    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]

    # ì…ë ¥ í† í°ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
    MAX_LEN = 128

    # í† í°ì„ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]
    
    # ë¬¸ì¥ì„ MAX_LEN ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³ , ëª¨ìë€ ë¶€ë¶„ì„ íŒ¨ë”© 0ìœ¼ë¡œ ì±„ì›€
    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

    # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì´ˆê¸°í™”
    attention_masks = []

    # ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ íŒ¨ë”©ì´ ì•„ë‹ˆë©´ 1, íŒ¨ë”©ì´ë©´ 0ìœ¼ë¡œ ì„¤ì •
    # íŒ¨ë”© ë¶€ë¶„ì€ BERT ëª¨ë¸ì—ì„œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ ì†ë„ í–¥ìƒ
    for seq in input_ids:
        seq_mask = [float(i>0) for i in seq]
        attention_masks.append(seq_mask)

    # ë°ì´í„°ë¥¼ íŒŒì´í† ì¹˜ì˜ í…ì„œë¡œ ë³€í™˜
    inputs = torch.tensor(input_ids)
    masks = torch.tensor(attention_masks)

    return inputs, masks
```


```python
# ë¬¸ì¥ í…ŒìŠ¤íŠ¸
def test_sentences(sentences):

    # í‰ê°€ëª¨ë“œë¡œ ë³€ê²½
    model.eval()

    # ë¬¸ì¥ì„ ì…ë ¥ ë°ì´í„°ë¡œ ë³€í™˜
    inputs, masks = convert_input_data(sentences)

    # ë°ì´í„°ë¥¼ GPUì— ë„£ìŒ
    b_input_ids = inputs.to(device)
    b_input_mask = masks.to(device)
            
    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì•ˆí•¨
    with torch.no_grad():     
        # Forward ìˆ˜í–‰
        outputs = model(b_input_ids, 
                        token_type_ids=None, 
                        attention_mask=b_input_mask)

    # ë¡œìŠ¤ êµ¬í•¨
    logits = outputs[0]

    # CPUë¡œ ë°ì´í„° ì´ë™
    logits = logits.detach().cpu().numpy()

    return logits
```


```python
logits = test_sentences(['ì—°ê¸°ëŠ” ë³„ë¡œì§€ë§Œ ì¬ë¯¸ í•˜ë‚˜ëŠ” ëë‚´ì¤Œ!'])

print(logits)
print(np.argmax(logits))
```


```python
logits = test_sentences(['ì£¼ì—°ë°°ìš°ê°€ ì•„ê¹ë‹¤. ì´ì²´ì  ë‚œêµ­...'])

print(logits)
print(np.argmax(logits))
```

í•™ìŠµí•œ ëª¨ë¸ì„ ê°€ì§€ê³  ì‹¤ì œ ë¬¸ì¥ì„ ë„£ì–´ë´¤ìŠµë‹ˆë‹¤. ì¶œë ¥ ë¡œì§“ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ê°€ ì ìš©ë˜ì§€ ì•Šì€ ìƒíƒœì…ë‹ˆë‹¤. argmaxë¡œ ë” ë†’ì€ ê°’ì˜ ìœ„ì¹˜ë¥¼ ë¼ë²¨ë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤. 0ì€ ë¶€ì •, 1ì€ ê¸ì •ì…ë‹ˆë‹¤. ìœ„ì™€ ê°™ì´ ìƒˆë¡œìš´ ë¬¸ì¥ì—ë„ ì˜ ë¶„ë¥˜ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
<br>
<br>
<br>

< ì±—ë´‡ ê°œë°œì ëª¨ì„ ><br>
- í˜ì´ìŠ¤ë¶ ê·¸ë£¹ì— ê°€ì…í•˜ì‹œë©´ ì±—ë´‡ì— ëŒ€í•œ ìµœì‹  ì •ë³´ë¥¼ ì‰½ê²Œ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- https://www.facebook.com/groups/ChatbotDevKR/
